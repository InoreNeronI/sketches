{"mappings":"AAAA;;CAEC,GACD,MAAM,SAAS,CACb,WACA,UACA,aACA,cACA,YACA,WACA,cACA,WACA,aACA,mBAAmB,IAAI;IAEvB,MAAM,KAAK,CAAC;IACZ,GAAG,SAAS,GAAG;IACf,GAAG,QAAQ,GAAG;IACd,GAAG,WAAW,GAAG;IACjB,GAAG,YAAY,GAAG;IAClB,GAAG,UAAU,GAAG;IAChB,GAAG,SAAS,GAAG;IACf,GAAG,YAAY,GAAG;IAClB,GAAG,SAAS,GAAG;IACf,GAAG,WAAW,GAAG;IACjB,GAAG,gBAAgB,GAAG;IAEtB,GAAG,SAAS,GAAG,IAAI;IACnB,KAAK,UAAU,CAAC,OAAO,CAAC,CAAC,GAAG;QAC1B,GAAG,SAAS,CAAC,GAAG,CAAC,GAAG;IACtB;IACA,KAAK,UAAU,CAAC,OAAO,CAAC,CAAC,GAAG;QAC1B,GAAG,SAAS,CAAC,GAAG,CAAC,GAAG;IACtB;IAEA,GAAG,MAAM,GAAG;IAEZ,GAAG,KAAK,GAAG,IAAO,GAAG,MAAM,GAAG;IAE9B;;;;GAIC,GACA,GAAG,UAAU,GAAG,CAAC,IAAI;QACpB,MAAM,OAAO,IAAM,MAAM,KAAK,UAAU,CAAC,MAAM,EAAE,IAAI,CAAC;QACtD,OAAO,MAAM,IAAI,CAAC,MAAM,KAAK,CAAC,IAAM,MAAM,IAAI,CAAC,MAAM,KAAK,CAAC,IAAM;IACnE,GACE;;KAEC,GACA,GAAG,SAAS,GAAG,CAAC,OAAS,GAAG,SAAS,CAAC,GAAG,CAAC,GAAG,cAAc,CAAC,QAC7D;;KAEC,GACA,GAAG,cAAc,GAAG,CAAC,OAAS,GAAG,KAAK,CAAC,MAAM,MAAM,CAAC,CAAC,KAAK,GAAG,GAAG,MAAS,IAAI,IAAK,IAAI,GAAG,CAAC,IAAI,GAAG,IAAI,MAAO,GAAI,IACjH;;KAEC,GACA,GAAG,oBAAoB,GAAG,IAAM,MAAM,OAAO,CAAC,GAAG,KAAK,UAAU,CAAC,MAAM,GAAG,IAC3E;;KAEC,GACA,GAAG,KAAK,GAAG,CAAC,OAAS,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,EAC/C;;KAEC,GACA,GAAG,IAAI,GAAG,CAAC,GAAG,OAAS,GAAG,KAAK,CAAC,KAAK,CAAC,EAAE,EACzC;;KAEC,GACA,GAAG,IAAI,GAAG,CAAC,GAAG,GAAG,OAAU,GAAG,MAAM,CAAC,KAAK,CAAC,CAAC,CAAC,KAAK,CAAC,CAAC,CAAC,EAAE,GAAG;IAE7D;;GAEC,GACA,GAAG,MAAM,GAAG,CAAC,OAAS,GAAG,KAAK,CAAC,MAAM,KAAK,CAAC,CAAC,GAAG,GAAG,MAAS,IAAI,IAAI,KAAK,GAAG,CAAC,IAAI,EAAE,GAAG,OACpF;;KAEC,GACA,GAAG,IAAI,GAAG,CAAC,OAAS,GAAG,KAAK,CAAC,MAAM,MAAM,CAAC,CAAC,KAAK,IAAO,MAAM,IAAI,MAAM,IACxE;;KAEC,GACA,GAAG,MAAM,GAAG,CAAC,MAAM;QAClB,IAAI,cAAc,GAAG,WAAW;QAChC,qCAAqC;QACrC,IAAI,GAAG,MAAM,IAAI,QAAQ,MAAM,SAAS,IAAI,CAAC,GAAG,gBAAgB,EAAE,GAAG,MAAM,GAAG,GAAG,UAAU,CAAC,MAAM,EAAE,EAAE,MAAM,EAAE;QAC9G,6EAA6E;QAC7E,IAAK,IAAI,KAAK,GAAG,KAAK,GAAG,SAAS,EAAE,KAAM;YACxC,IAAI,IAAI;YACR,yEAAyE;YACzE,IAAK,IAAI,OAAO,GAAG,OAAO,UAAU,OAAQ;gBAC1C,MAAM,OAAO,EAAE,KAAK,CAAC,EAAE;gBACvB,2BAA2B;gBAC3B,MAAM,KAAK,GAAG,SAAS,CAAC,MAAM,eAAe,GAAG,oBAAoB,KAAK,GAAG,cAAc,CAAC;gBAC3F,MAAM,KAAK,KAAK,GAAG;oBAAE,WAAW,GAAG,SAAS,CAAC,GAAG,CAAC;gBAAI;gBACrD,4CAA4C;gBAC5C,MAAM,IAAI,CAAC,GAAG,OAAO,GAAG,GAAG,WAAW,GAAG,GAAG,SAAS,GAAG,GAAG,SAAS,GAAG;gBACvE,2DAA2D;gBAC3D,MAAM,KAAK,GAAG,KAAK,CAAC,GAAG,IAAI,CAAC,IAAI,OAAO,GAAG,GAAG,IAAI,CAAC,GAAG,KAAK,CAAC,EAAE;gBAC7D,GAAG,IAAI,CAAC,IAAI,IAAI;gBAChB,mFAAmF;gBACnF,IAAI,CAAC,GAAG,OAAO,IAAI,GAAG,SAAS,EAAE;qBAC5B,IAAI;YACX;YACA,gFAAgF;YAChF,cAAc,KAAK,GAAG,CAAC,GAAG,UAAU,EAAE,cAAc,GAAG,YAAY;QACrE;IACF;IAEF;;;GAGC,GACD,GAAG,KAAK,GAAG,CAAC,GAAG,GAAG,KAAO,IAAI,GAAG,SAAS,GAAI,CAAA,IAAI,GAAG,YAAY,GAAG,KAAK,CAAA;IAExE;;GAEC,GACD,GAAG,SAAS,GAAG,CAAC,MAAM,cAAgB,KAAK,MAAM,KAAK,eAAe,GAAG,MAAM,CAAC;IAE/E,OAAO;AACT","sources":["src/snake-pro/src/q-learning.js"],"sourcesContent":["/**\n *\n */\nconst QLearn = (\n  nEpisodes,\n  maxSteps,\n  exploreRate,\n  exploreDecay,\n  exploreMin,\n  learnRate,\n  discountRate,\n  eatReward,\n  deathReward,\n  cumulativePolicy = true,\n) => {\n  const ql = {};\n  ql.nEpisodes = nEpisodes;\n  ql.maxSteps = maxSteps;\n  ql.exploreRate = exploreRate;\n  ql.exploreDecay = exploreDecay;\n  ql.exploreMin = exploreMin;\n  ql.learnRate = learnRate;\n  ql.discountRate = discountRate;\n  ql.eatReward = eatReward;\n  ql.deathReward = deathReward;\n  ql.cumulativePolicy = cumulativePolicy;\n\n  ql.actionMap = new Map();\n  game.DIRECTIONS.forEach((d, i) => {\n    ql.actionMap.set(i, d);\n  });\n  game.DIRECTIONS.forEach((d, i) => {\n    ql.actionMap.set(d, i);\n  });\n\n  ql.policy = null;\n\n  ql.reset = () => (ql.policy = null);\n\n  /**\n   * Returns a policy matrix filled with 0s\n   * Policy is a 3D matrix of x columns, y rows, each [x,y] contains\n   * an value is the predicted reward for its corresponding action (direction)\n   */\n  (ql.initPolicy = (nx, ny) => {\n    const mkQs = () => Array(game.DIRECTIONS.length).fill(0);\n    return Array.from(Array(nx), (_) => Array.from(Array(ny), (_) => mkQs()));\n  }),\n    /**\n     * Returns the best direction to move in for a given position in the policy\n     */\n    (ql.getAction = (node) => ql.actionMap.get(ql.getActionIndex(node))),\n    /**\n     * Returns the best action index for a given position in the policy\n     */\n    (ql.getActionIndex = (node) => ql.getQs(node).reduce((acc, v, i, arr) => (i > 0 ? (v > arr[acc] ? i : acc) : i), 0)),\n    /**\n     * Returns a random action index\n     */\n    (ql.getRandomActionIndex = () => utils.randInt(0, game.DIRECTIONS.length - 1)),\n    /**\n     * Returns the Q for a specified location and action\n     */\n    (ql.getQs = (node) => ql.policy[node.x][node.y]),\n    /**\n     * Returns the Q for a specified location and action index\n     */\n    (ql.getQ = (i, node) => ql.getQs(node)[i]),\n    /**\n     * Sets the Q at a specified location for a specified action index\n     */\n    (ql.setQ = (i, q, node) => (ql.policy[node.x][node.y][i] = q));\n\n  /**\n   * Returns true if all Q in a given position are equal\n   */\n  (ql.allQEq = (node) => ql.getQs(node).every((q, i, arr) => (i > 0 ? q == arr[i - 1] : true))),\n    /**\n     * Returns the maximum Q at a given position\n     */\n    (ql.maxQ = (node) => ql.getQs(node).reduce((acc, v) => (acc > v ? acc : v))),\n    /**\n     * Updates the policy resulting from the Q-Learning algorithm based on the given state\n     */\n    (ql.update = (next, state) => {\n      let exploreRate = ql.exploreRate;\n      // If no policy is given, start fresh\n      if (ql.policy == null || state.justEaten || !ql.cumulativePolicy) ql.policy = ql.initPolicy(state.nx, state.ny);\n      // Play the game nEpisodes times from the current start to gather information\n      for (let ep = 0; ep < ql.nEpisodes; ep++) {\n        let s = state;\n        // Each episode is limited to a maximum number of steps that can be taken\n        for (let step = 0; step < maxSteps; step++) {\n          const head = s.snake[0];\n          // Get an action to try out\n          const ai = ql.isExplore(head, exploreRate) ? ql.getRandomActionIndex() : ql.getActionIndex(head);\n          const ns = next(s, { direction: ql.actionMap.get(ai) });\n          // Get reward based on outcome of the action\n          const r = !ns.isAlive ? ql.deathReward : ns.justEaten ? ql.eatReward : 0;\n          // Calculate and update the Q for the current head location\n          const nQ = ql.calcQ(ql.getQ(ai, head), r, ql.maxQ(ns.snake[0]));\n          ql.setQ(ai, nQ, head);\n          // If the snake is dead or just ate, end the episode early, otherwise advance state\n          if (!ns.isAlive || ns.justEaten) break;\n          else s = ns;\n        }\n        // Exploration rate decays linearly with each episode until a minimum is reached\n        exploreRate = Math.max(ql.exploreMin, exploreRate - ql.exploreDecay);\n      }\n    });\n\n  /**\n   * Return an updated q value for the current node\n   * Using the Bellman equation\n   */\n  ql.calcQ = (q, r, nQ) => q + ql.learnRate * (r + ql.discountRate * nQ - q);\n\n  /**\n   * Returns true if the next action should be to explore\n   */\n  ql.isExplore = (node, exploreRate) => Math.random() < exploreRate || ql.allQEq(node);\n\n  return ql;\n};\n"],"names":[],"version":3,"file":"index.36260a3a.js.map"}