{"mappings":"","sources":["src/snake-pro/src/q-learning.js"],"sourcesContent":["/**\n *\n */\nconst QLearn = (\n  nEpisodes,\n  maxSteps,\n  exploreRate,\n  exploreDecay,\n  exploreMin,\n  learnRate,\n  discountRate,\n  eatReward,\n  deathReward,\n  cumulativePolicy = true,\n) => {\n  const ql = {};\n  ql.nEpisodes = nEpisodes;\n  ql.maxSteps = maxSteps;\n  ql.exploreRate = exploreRate;\n  ql.exploreDecay = exploreDecay;\n  ql.exploreMin = exploreMin;\n  ql.learnRate = learnRate;\n  ql.discountRate = discountRate;\n  ql.eatReward = eatReward;\n  ql.deathReward = deathReward;\n  ql.cumulativePolicy = cumulativePolicy;\n\n  ql.actionMap = new Map();\n  game.DIRECTIONS.forEach((d, i) => {\n    ql.actionMap.set(i, d);\n  });\n  game.DIRECTIONS.forEach((d, i) => {\n    ql.actionMap.set(d, i);\n  });\n\n  ql.policy = null;\n\n  ql.reset = () => (ql.policy = null);\n\n  /**\n   * Returns a policy matrix filled with 0s\n   * Policy is a 3D matrix of x columns, y rows, each [x,y] contains\n   * an value is the predicted reward for its corresponding action (direction)\n   */\n  (ql.initPolicy = (nx, ny) => {\n    const mkQs = () => Array(game.DIRECTIONS.length).fill(0);\n    return Array.from(Array(nx), (_) => Array.from(Array(ny), (_) => mkQs()));\n  }),\n    /**\n     * Returns the best direction to move in for a given position in the policy\n     */\n    (ql.getAction = (node) => ql.actionMap.get(ql.getActionIndex(node))),\n    /**\n     * Returns the best action index for a given position in the policy\n     */\n    (ql.getActionIndex = (node) =>\n      ql.getQs(node).reduce((acc, v, i, arr) => (i > 0 ? (v > arr[acc] ? i : acc) : i), 0)),\n    /**\n     * Returns a random action index\n     */\n    (ql.getRandomActionIndex = () => utils.randInt(0, game.DIRECTIONS.length - 1)),\n    /**\n     * Returns the Q for a specified location and action\n     */\n    (ql.getQs = (node) => ql.policy[node.x][node.y]),\n    /**\n     * Returns the Q for a specified location and action index\n     */\n    (ql.getQ = (i, node) => ql.getQs(node)[i]),\n    /**\n     * Sets the Q at a specified location for a specified action index\n     */\n    (ql.setQ = (i, q, node) => (ql.policy[node.x][node.y][i] = q));\n\n  /**\n   * Returns true if all Q in a given position are equal\n   */\n  (ql.allQEq = (node) => ql.getQs(node).every((q, i, arr) => (i > 0 ? q == arr[i - 1] : true))),\n    /**\n     * Returns the maximum Q at a given position\n     */\n    (ql.maxQ = (node) => ql.getQs(node).reduce((acc, v) => (acc > v ? acc : v))),\n    /**\n     * Updates the policy resulting from the Q-Learning algorithm based on the given state\n     */\n    (ql.update = (next, state) => {\n      let exploreRate = ql.exploreRate;\n      // If no policy is given, start fresh\n      if (ql.policy == null || state.justEaten || !ql.cumulativePolicy) ql.policy = ql.initPolicy(state.nx, state.ny);\n      // Play the game nEpisodes times from the current start to gather information\n      for (let ep = 0; ep < ql.nEpisodes; ep++) {\n        let s = state;\n        // Each episode is limited to a maximum number of steps that can be taken\n        for (let step = 0; step < maxSteps; step++) {\n          const head = s.snake[0];\n          // Get an action to try out\n          const ai = ql.isExplore(head, exploreRate) ? ql.getRandomActionIndex() : ql.getActionIndex(head);\n          const ns = next(s, { direction: ql.actionMap.get(ai) });\n          // Get reward based on outcome of the action\n          const r = !ns.isAlive ? ql.deathReward : ns.justEaten ? ql.eatReward : 0;\n          // Calculate and update the Q for the current head location\n          const nQ = ql.calcQ(ql.getQ(ai, head), r, ql.maxQ(ns.snake[0]));\n          ql.setQ(ai, nQ, head);\n          // If the snake is dead or just ate, end the episode early, otherwise advance state\n          if (!ns.isAlive || ns.justEaten) break;\n          else s = ns;\n        }\n        // Exploration rate decays linearly with each episode until a minimum is reached\n        exploreRate = Math.max(ql.exploreMin, exploreRate - ql.exploreDecay);\n      }\n    });\n\n  /**\n   * Return an updated q value for the current node\n   * Using the Bellman equation\n   */\n  ql.calcQ = (q, r, nQ) => q + ql.learnRate * (r + ql.discountRate * nQ - q);\n\n  /**\n   * Returns true if the next action should be to explore\n   */\n  ql.isExplore = (node, exploreRate) => Math.random() < exploreRate || ql.allQEq(node);\n\n  return ql;\n};\n"],"names":[],"version":3,"file":"index.26e7512a.js.map"}